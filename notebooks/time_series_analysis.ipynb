{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "Before we start that we need to make sure all data is preprocessed and all columns have the right name!\n",
    "Of course as usuall we start by importing relevant libraries packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "#tensorflow or pytorch!\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set()\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"../features/train_features.csv\")\n",
    "test_features = pd.read_csv(\"../features/test_features.csv\")\n",
    "train_sales = pd.read_csv(\"../features/train_sales.csv\")\n",
    "train_customers = pd.read_csv(\"../features/train_customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 844392 entries, 0 to 844391\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   Sales   844392 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all our features are numeric. Since our problem is a regression problem, we can narrow down the list of algorithms we will use for modelling. Random forests Regressor will make for a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "No feature_importances found. Need to call fit beforehand.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_features]\n\u001b[0;32m      3\u001b[0m feature_columns\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\rafaa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\lightgbm\\sklearn.py:874\u001b[0m, in \u001b[0;36mLGBMModel.feature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m\"\"\":obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    to configure the type of importance values to be extracted.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m--> 874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo feature_importances found. Need to call fit beforehand.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mfeature_importance(importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_type)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: No feature_importances found. Need to call fit beforehand."
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(train_sales)\n",
    "feature_columns = [x for x in train_features]\n",
    "feature_columns\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "# regressor.fit(X_train, y_train)\n",
    "# y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "# print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "# print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
